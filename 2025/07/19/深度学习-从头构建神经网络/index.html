<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>深度学习：从头构建神经网络 | Carvino's Blog</title>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="">
  <meta name="theme-color" content="#10b981">

  <link rel="canonical" href="https://carvinozheng.github.io/2025/07/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

  
    <link rel="shortcut icon" href="/images/icon.png">
  

  <meta name="description" content="Description本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。 该模型将足够简单，大家都能使用CPU或GPU来构建和训练。 这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。 准备数据假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示： $ y &#x3D; w_1 * x_1">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习：从头构建神经网络">
<meta property="og:url" content="https://carvinozheng.github.io/2025/07/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Carvino&#39;s Blog">
<meta property="og:description" content="Description本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。 该模型将足够简单，大家都能使用CPU或GPU来构建和训练。 这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。 准备数据假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示： $ y &#x3D; w_1 * x_1">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-19T09:46:01.000Z">
<meta property="article:modified_time" content="2025-07-24T04:38:44.953Z">
<meta property="article:author" content="fishcanf1y">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
  <meta name="generator" content="Hexo 7.3.0">
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://carvinozheng.github.io/2025/07/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},"headline":"深度学习：从头构建神经网络","description":"Description本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。 该模型将足够简单，大家都能使用CPU或GPU来构建和训练。 这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。 准备数据假设我们要训练一个具有四个权重","datePublished":"2025-07-19T09:46:01.000Z","dateModified":"2025-07-24T04:38:44.953Z","author":{"@type":"Person","name":"fishcanf1y"},"publisher":{"@type":"Organization","name":"Carvino's Blog","logo":{"@type":"ImageObject","url":"https://carvinozheng.github.io/images/icon.png"}}}
</script>

  
  
<link rel="stylesheet" href="/css/main.css">

  <style>
    :root {
      --sea-color-primary: #10b981;
    }
  </style>

  
<script src="/js/theme_mode.js"></script>

<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body>
  <header class="sea-header">
    <nav class="sea-nav-wrap">
  <div class="sea-nav-logo" title="">
    <a href="/">Carvino's Blog</a>
  </div>
  <div class="sea-nav-menus">
    <div id="sea-nav-toggle">
      <svg t="1716965724278" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10878" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M950.857143 768v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429v-73.142857c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z m0-292.571429v73.142858c0 20.004571-16.566857 36.571429-36.571429 36.571428H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571428v-73.142858c0-20.004571 16.566857-36.571429 36.571429-36.571428h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571428z m0-292.571428v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429V182.857143c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z" p-id="10879"></path></svg>
    </div>

    <div id="sea-nav-dimmer"></div>
<div class="sea-menu-wrap">
  
    <a
      class="sea-menu-link "
      
      href="/"
    >
      Home
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/archives/"
    >
      Archives
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/friends/"
    >
      Friends
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/messages/"
    >
      Messages
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/about/"
    >
      About
    </a>
  

  <span class="sea-menu-sep">|</span>

  
  

  


  <span class="sea-menu-icon" id="sea-theme-dark">
    <svg t="1725413107294" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10118" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M557.553778 976.355556c-257.265778 0-466.56-207.160889-466.56-464.426667 0-253.923556 206.577778-464.284444 460.501333-464.284445h0.355556c10.766222 0 20.622222 3.953778 25.443555 13.610667 4.878222 9.756444 3.740444 20.394667-2.915555 29.027556-55.722667 72.220444-85.162667 158.108444-85.162667 249.372444 0 225.891556 183.779556 409.386667 409.671111 409.386667l5.248-0.256c10.325333-0.142222 20.977778 5.859556 25.841778 15.644444a28.302222 28.302222 0 0 1-2.915556 30.051556C837.902222 910.08 703.203556 976.355556 557.553778 976.355556zM495.274667 105.016889C299.192889 135.281778 147.882667 306.161778 147.882667 509.809778c0 225.877333 183.779556 409.656889 409.671111 409.656889 108.686222 0 210.403556-42.055111 286.577778-116.977778-231.566222-27.192889-411.804444-224.625778-411.804445-463.36 0-83.427556 21.617778-163.299556 62.947556-234.112z" fill="" p-id="10119"></path><path d="M578.830222 879.132444c-186.865778 0-345.784889-133.418667-377.841778-317.269333a14.222222 14.222222 0 1 1 28.017778-4.878222c29.681778 170.183111 176.810667 293.703111 349.824 293.703111a14.222222 14.222222 0 1 1 0 28.444444zM209.991111 531.2c-7.537778 0-13.838222-6.997333-14.193778-14.606222-0.312889-6.584889-0.483556-13.795556-0.483555-20.465778 0-7.864889 6.357333-14.492444 14.222222-14.492444s14.222222 6.229333 14.222222 14.094222c0 6.229333 0.170667 13.425778 0.455111 19.584 0.369778 7.850667-5.674667 15.886222-13.525333 15.886222h-0.696889z" fill="" p-id="10120"></path><path d="M622.350222 309.930667m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10121"></path><path d="M787.072 188.273778m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10122"></path><path d="M731.960889 415.303111m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" p-id="10123"></path></svg>
  </span>
  <span class="sea-menu-icon" id="sea-theme-light">
    <svg t="1725410359322" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4274" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M512 768c-141.376 0-256-114.624-256-256s114.624-256 256-256 256 114.624 256 256-114.624 256-256 256z m0-85.333333a170.666667 170.666667 0 1 0 0-341.333334 170.666667 170.666667 0 0 0 0 341.333334zM469.333333 85.333333a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0V85.333333z m0 768a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0v-85.333334zM85.333333 554.666667a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334H85.333333z m768 0a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334h-85.333334zM161.834667 222.165333a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m576 576a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m-515.669334 64a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z m576-576a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z" p-id="4275"></path></svg>
  </span>

  <span id="sea-menu-close-icon">
    <svg t="1725435896874" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4408" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M556.8 512l265.6-265.6c12.8-12.8 12.8-32 0-44.8s-32-12.8-44.8 0L512 467.2 246.4 201.6c-12.8-12.8-32-12.8-44.8 0s-12.8 32 0 44.8l265.6 265.6-265.6 265.6c-12.8 12.8-12.8 32 0 44.8 6.4 6.4 12.8 9.6 22.4 9.6s16-3.2 22.4-9.6l265.6-265.6 265.6 265.6c6.4 6.4 16 9.6 22.4 9.6s16-3.2 22.4-9.6c12.8-12.8 12.8-32 0-44.8L556.8 512z" p-id="4409"></path></svg>
  </span>
</div>
  </div>
</nav>
  </header>
  <main id="sea-main-wrapper" data-pagefind-body>
    <article class="sea-page-card-wrapper">
  <header class="sea-article-header">
    <h1 class="sea-article-title">深度学习：从头构建神经网络</h1>
    
      <div class="sea-post-meta sea-post-meta__center">
        <div class="sea-post-time">
  <svg t="1716964550804" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2621" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M805.49888 981.49888l-602.3168-0.76288c-86.59456-8.192-154.56768-81.3056-154.56768-170.01472L48.6144 291.73248c0-94.1568 76.60032-170.75712 170.7776-170.75712l586.10176 0c94.1568 0 170.73152 76.60032 170.73152 170.75712L976.22528 810.7008C976.2304 904.87296 899.63008 981.49888 805.49888 981.49888L805.49888 981.49888zM219.3664 190.57152c-55.79776 0-101.20192 45.38368-101.20192 101.18144l0 518.96832c0 55.79776 45.40416 101.20704 101.20192 101.20704l586.13248 0c55.77728 0 101.16096-45.40928 101.16096-101.20704L906.65984 291.73248c0-55.79776-45.38368-101.18656-101.16096-101.18656L219.3664 190.54592 219.3664 190.57152zM698.84416 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L745.22624 244.1216C745.22624 269.7472 724.46976 290.51904 698.84416 290.51904L698.84416 290.51904zM315.65824 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L362.04032 244.1216C362.04032 269.7472 341.28896 290.51904 315.65824 290.51904L315.65824 290.51904zM534.8864 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C581.26848 774.01088 560.4864 794.78784 534.8864 794.78784L534.8864 794.78784zM930.79552 452.608 121.24672 452.608c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l809.5744 0c25.6 0 46.38208 20.77696 46.38208 46.38208C977.2032 431.82592 956.42624 452.608 930.79552 452.608L930.79552 452.608zM327.92576 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208C374.30784 628.25472 353.52576 649.03168 327.92576 649.03168L327.92576 649.03168zM534.8864 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208S560.4864 649.03168 534.8864 649.03168L534.8864 649.03168zM741.27872 649.03168l-44.26752 0c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.77696 46.38208 46.38208C787.6608 628.25472 766.90944 649.03168 741.27872 649.03168L741.27872 649.03168zM327.92576 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C374.30784 774.01088 353.52576 794.78784 327.92576 794.78784L327.92576 794.78784zM741.27872 794.78784l-44.26752 0c-25.60512 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.78208 46.38208 46.38208C787.6608 774.01088 766.90944 794.78784 741.27872 794.78784L741.27872 794.78784z" p-id="2622"></path></svg>
  <time datetime="2025-07-19T09:46:01.000Z">2025-07-19</time>
</div>
        
  <div class="sea-post-categories">
    <svg t="1716964680422" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4550" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M810.666667 85.333333a85.333333 85.333333 0 0 1 85.333333 85.333334v152.021333c36.821333 9.493333 64 42.88 64 82.645333v405.333334a128 128 0 0 1-128 128H192a128 128 0 0 1-128-128V298.666667a85.376 85.376 0 0 1 64-82.645334V170.666667a85.333333 85.333333 0 0 1 85.333333-85.333334h597.333334zM128.149333 296.170667L128 298.666667v512a64 64 0 0 0 60.245333 63.893333L192 874.666667h640a64 64 0 0 0 63.893333-60.245334L896 810.666667V405.333333a21.333333 21.333333 0 0 0-18.837333-21.184L874.666667 384H638.165333l-122.069333-101.717333a21.333333 21.333333 0 0 0-10.688-4.736l-2.986667-0.213334H149.333333a21.333333 21.333333 0 0 0-21.184 18.837334zM535.189333 213.333333l127.978667 106.666667H832V170.666667a21.333333 21.333333 0 0 0-18.837333-21.184L810.666667 149.333333H213.333333a21.333333 21.333333 0 0 0-21.184 18.837334L192 170.666667v42.666666h343.168z" p-id="4551"></path></svg>
    <a class="category-link" href="/categories/AI/">AI</a>
  </div>

        
  <div class="sea-post-tags">
    <svg t="1716964811431" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6117" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M384 977.152c-20.5312 0-39.8336-7.9872-54.3232-22.4256l-260.4032-260.4032c-14.4896-14.4896-22.4256-33.7408-22.4256-54.3232s7.9872-39.8336 22.4256-54.3232l439.6032-439.6032c24.9344-24.9344 70.2464-43.7248 105.5232-43.7248h230.4c42.3424 0 76.8 34.4576 76.8 76.8v230.4c0 35.2256-18.7904 80.5888-43.6736 105.5232l-439.6032 439.6032a76.1856 76.1856 0 0 1-54.3232 22.4256zM614.4 153.6c-21.248 0-54.272 13.6704-69.2736 28.7232l-439.6032 439.6032c-4.8128 4.8128-7.424 11.2128-7.424 18.1248s2.6624 13.312 7.424 18.0736l260.4032 260.4032c4.8128 4.8128 11.2128 7.424 18.1248 7.424s13.312-2.6624 18.1248-7.424l439.6032-439.6032c15.0016-15.0016 28.7232-48.0768 28.7232-69.3248V179.2a25.6 25.6 0 0 0-25.6-25.6h-230.4z" p-id="6118"></path><path d="M742.4 358.4c-42.3424 0-76.8-34.4576-76.8-76.8S700.0576 204.8 742.4 204.8s76.8 34.4576 76.8 76.8S784.7424 358.4 742.4 358.4z m0-102.4a25.6 25.6 0 1 0 0 51.2 25.6 25.6 0 0 0 0-51.2z" p-id="6119"></path></svg>
    <a class="tag-link" href="/tags/AI/" rel="tag">AI</a> , <a class="tag-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a>
  </div>

      </div>
    
  </header>
  <div class="sea-doc">
    
    <div class="sea-article-content">
      <h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>本文旨在使用PyTorch构建并训练一个最简单的神经网络，无需添加任何花哨的层或依赖包。</p>
<p>该模型将足够简单，大家都能使用CPU或GPU来构建和训练。</p>
<p>这个模型虽然简单，但包含了当前诸如LLM和Stable Diffusions等大型模型所拥有的所有基本元素。</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>假设我们要训练一个具有四个权重并能输出一个数字结果的模型，如下所示：</p>
<p>$ y &#x3D; w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 $</p>
<p>我们先生成以下训练数据，假设权重值为 [2,3,4,7]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">w_list = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">7</span>])</span><br></pre></td></tr></table></figure>

<p>我们的模型将会用于预测权重列表，因此再生成一些训练数据后我们假装我们不知道这些权重的值</p>
<p>之后我们创建10组输入样本数据——x_sample，每组x_sample是一个包含4个元素的数组，与权重的长度相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate 10 random input samples (each with same length as w_list)</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br></pre></td></tr></table></figure>

<p>这里，我们使用numpy，因为我们想利用numpy的点积函数轻松生成输出——y</p>
<p>说到y，我们来生成一个同样包含10个元素的y_list：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list </span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们的训练数据已准备就绪，无需下载任何内容，也无需使用DataLoader等，接下来，我们可以开始定义模型</p>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>我们的模型可能是世界上最简单的模型，即以下代码中定义的一个简单的线性点积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">​    <span class="built_in">super</span>().__init__()</span><br><span class="line">​    <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">​    <span class="built_in">print</span>(<span class="string">&quot;Initial weights:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># Print initialized weights</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">​    <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br></pre></td></tr></table></figure>

<p>在上述代码中，使用self.w &#x3D; nn.Parameter(torch.randn(len(w_list)))初始化了权重张量。</p>
<p>无需其他代码，我们的神经网络模型现已准备就绪，命名为——MyLinear</p>
<h2 id="准备训练模型"><a href="#准备训练模型" class="headerlink" title="准备训练模型"></a>准备训练模型</h2><p>我们需要像LLM那样初始化模型的随机权重</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = MyLinear()</span><br></pre></td></tr></table></figure>

<p>几乎所有神经网络模型的训练都遵循以下步骤：</p>
<ul>
<li>前向传播以预测结果</li>
<li>与真实值进行比较以获取损失值</li>
<li>反向传播梯度损失值</li>
<li>更新模型参数</li>
</ul>
<p>因此，在开始训练之前，我们需要定义一个<code>损失函数</code>和一个<code>优化器</code></p>
<p>损失函数<code>loss_fn</code>将根据预测结果和真实结果计算损失值，优化器将用于更新权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br></pre></td></tr></table></figure>

<p>lr代表学习率，这是最难设置的超参数之一，确定最佳学习率（lr）通常需要根据模型、数据集和问题的特性进行反复试验。然不过有一些策略和技术可以帮助估计一个合理的学习率</p>
<ul>
<li>从较小的学习率开始：一种常见做法是从较小的学习率（如0.001）开始，并根据观察到的收敛行为逐渐增加或减少它</li>
<li>学习率调度：可以使用学习率调度在训练过程中动态调整学习率。一种常见方法是阶梯衰减，即在固定数量的训练周期后降低学习率。另一种流行方法是指数衰减，即学习率随时间呈指数下降</li>
</ul>
<p>另外别忘了将输入和输出转换为torch张量对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)</span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)</span><br></pre></td></tr></table></figure>



<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>我们将训练周期数设置为100，这意味着我们将遍历训练数据100次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 开始训练模型</span><br><span class="line">num_epochs = 100  # 总训练周期</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for i, x in enumerate(x_input):</span><br><span class="line">        # 向前传播</span><br><span class="line">        y_pred = model(x)  # Get prediction from model</span><br><span class="line">        </span><br><span class="line">        # 计算损失值</span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  # Compare prediction with true value</span><br><span class="line">        </span><br><span class="line">        # 清除之前的缓存的梯度参数</span><br><span class="line">        optimizer.zero_grad()  # Clear previous gradient information</span><br><span class="line">        </span><br><span class="line">        # 反向传播计算梯度</span><br><span class="line">        loss.backward()  # Compute gradients for current parameters</span><br><span class="line">        </span><br><span class="line">        # 更新模型参数</span><br><span class="line">        optimizer.step()  # Update model parameters based on gradients</span><br><span class="line">    </span><br><span class="line">    # 每十个epoch打印一次训练进度</span><br><span class="line">    if (epoch+1) % 10 == 0:</span><br><span class="line">        print(f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">print(&quot;Training completed&quot;)</span><br></pre></td></tr></table></figure>

<p>在上述代码中，我们可以看到两层循环，外层循环用于训练周期，内层循环用于遍历10组样本数据</p>
<p>我们有10组输入，每组输入有4个元素，即x_1, x_2, x_3, 和 x_4</p>
<p>正如我们在准备阶段所讨论的，第一步是使用模型预测一个结果：y_pred，然后，调用loss_fn来计算损失值</p>
<p>在将损失值反向传播之前，我们需要通过调用optimizer.zero_grad()来清除上一次的梯度值</p>
<p>最后，调用backward和optimizer.step()来更新模型参数</p>
<p>运行代码，我们将看到程序输出类似于以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch [10/100], Loss: 218.3843</span><br><span class="line">Epoch [20/100], Loss: 283.2002</span><br><span class="line">Epoch [30/100], Loss: 116.5593</span><br><span class="line">Epoch [40/100], Loss: 43.8340</span><br><span class="line">Epoch [50/100], Loss: 16.3244</span><br><span class="line">Epoch [60/100], Loss: 6.0721</span><br><span class="line">Epoch [70/100], Loss: 2.2586</span><br><span class="line">Epoch [80/100], Loss: 0.8400</span><br><span class="line">Epoch [90/100], Loss: 0.3124</span><br><span class="line">Epoch [100/100], Loss: 0.1162</span><br><span class="line">train done</span><br></pre></td></tr></table></figure>
<p>随着时间的推移，损失值在下降，这看起来很不错，我们在代码中添加以下功能：</p>
<ul>
<li><p>检查模型当前权重</p>
</li>
<li><p>增加权重数量(上述案例中我们只用了4个权重)</p>
</li>
<li><p>增加训练周期数和更新学习率——lr，</p>
</li>
</ul>
<p>以下是整理出的完整代码(无CUDA版) 同时改动了权重数量和训练周期</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"><span class="comment"># ----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义真实的权重向量（训练目标）</span></span><br><span class="line">w_list = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">5</span>, <span class="number">13</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成10个随机输入样本（每个样本长度与w_list相同）</span></span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x_sample = np.array([random.randint(<span class="number">1</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_list))])</span><br><span class="line">    x_list.append(x_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算对应的目标输出（输入与真实权重的点积）</span></span><br><span class="line">y_list = []</span><br><span class="line"><span class="keyword">for</span> x_sample <span class="keyword">in</span> x_list:</span><br><span class="line">    y_temp = x_sample @ w_list  <span class="comment"># 矩阵乘法计算预测值</span></span><br><span class="line">    y_list.append(y_temp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 准备训练环境</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自定义线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 初始化可学习参数（长度与w_list相同）</span></span><br><span class="line">        <span class="variable language_">self</span>.w = nn.Parameter(torch.randn(<span class="built_in">len</span>(w_list), dtype=torch.float32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;初始权重:&quot;</span>, <span class="variable language_">self</span>.w)  <span class="comment"># 打印初始化的权重</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># 前向传播：权重向量与输入向量的点积</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w @ x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型（CPU模式）</span></span><br><span class="line">model = MyLinear()  <span class="comment"># 不使用GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数（均方误差）</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器（随机梯度下降，学习率0.00001）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理（CPU模式）</span></span><br><span class="line">x_input = torch.tensor(x_list, dtype=torch.float32)  <span class="comment"># 输入数据转为张量</span></span><br><span class="line">y_output = torch.tensor(y_list, dtype=torch.float32)  <span class="comment"># 目标数据转为张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 开始模型训练</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------</span></span><br><span class="line">num_epochs = <span class="number">200</span>  <span class="comment"># 总训练轮数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(x_input):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        y_pred = model(x)  <span class="comment"># 获取模型预测值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = loss_fn(y_pred, y_output[i])  <span class="comment"># 比较预测值与真实值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 清除梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清除之前的梯度信息</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()  <span class="comment"># 计算当前参数的梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 根据梯度更新模型参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每10轮打印一次训练进度</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;轮次 [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], 损失: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终权重:&quot;</span>, model.w.detach().numpy())</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<p>初始模型权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初始权重: Parameter containing:</span><br><span class="line">tensor([-0.3772, -1.7161,  0.2819, -0.4298, -0.8599, -0.2690, -0.5375],</span><br></pre></td></tr></table></figure>

<p>模型计算出的损失值和权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">轮次 [10/200], 损失: 223.6982</span><br><span class="line">轮次 [20/200], 损失: 3.3349</span><br><span class="line">轮次 [30/200], 损失: 42.0250</span><br><span class="line">轮次 [40/200], 损失: 41.3786</span><br><span class="line">轮次 [50/200], 损失: 26.6852</span><br><span class="line">轮次 [60/200], 损失: 14.8007</span><br><span class="line">轮次 [70/200], 损失: 7.7069</span><br><span class="line">轮次 [80/200], 损失: 3.9039</span><br><span class="line">轮次 [90/200], 损失: 1.9543</span><br><span class="line">轮次 [100/200], 损失: 0.9728</span><br><span class="line">轮次 [110/200], 损失: 0.4838</span><br><span class="line">轮次 [120/200], 损失: 0.2403</span><br><span class="line">轮次 [130/200], 损失: 0.1195</span><br><span class="line">轮次 [140/200], 损失: 0.0594</span><br><span class="line">轮次 [150/200], 损失: 0.0295</span><br><span class="line">轮次 [160/200], 损失: 0.0147</span><br><span class="line">轮次 [170/200], 损失: 0.0073</span><br><span class="line">轮次 [180/200], 损失: 0.0036</span><br><span class="line">轮次 [190/200], 损失: 0.0018</span><br><span class="line">轮次 [200/200], 损失: 0.0009</span><br><span class="line">训练完成</span><br><span class="line">最终权重: [ 2.003787   3.0008602  3.9975853  6.9999294 10.997534   5.0006266</span><br><span class="line"> 12.999097 ]</span><br></pre></td></tr></table></figure>

<p>我们代码中所定义的权重：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 定义真实的权重向量（训练目标）</span><br><span class="line">w_list = np.array([2, 3, 4, 7, 11, 5, 13])</span><br></pre></td></tr></table></figure>

<p>从中我们可以看到我们的模型最终计算出的权重值非常[2,3,4,7,11,5,13]，由此得出模型已成功训练并找到了正确的权重值！</p>

    </div>
  </div>

  
    
  <div class="sea-prev-next-wrapper">
    
      <div class="prev">
        <svg t="1725418977480" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4239" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="4240"></path></svg>
        <a class="link" href="/2025/07/21/%E6%94%BB%E9%98%B2%E4%B8%96%E7%95%8CCrypto-baigeiRSA/">
          攻防世界Crypto-baigeiRSA
        </a>
      </div>
    
    
      <div class="next">
        <a class="link" href="/2025/07/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%9701-%E6%A5%94%E5%AD%90/">
          动手学深度学习系列01-楔子
        </a>
        <svg t="1725418993065" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6832" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M731.733333 480l-384-341.333333c-17.066667-14.933333-44.8-14.933333-59.733333 4.266666-14.933333 17.066667-14.933333 44.8 4.266667 59.733334L640 512 292.266667 821.333333c-17.066667 14.933333-19.2 42.666667-4.266667 59.733334 8.533333 8.533333 19.2 14.933333 32 14.933333 10.666667 0 19.2-4.266667 27.733333-10.666667l384-341.333333c8.533333-8.533333 14.933333-19.2 14.933334-32s-4.266667-23.466667-14.933334-32z" p-id="6833"></path></svg>
      </div>
    
  </div>

  
</article>


  

  


<script defer>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleIcon = document.querySelector('.sea-article-catalog-title .sea-svg-icon');
    const tocContent = document.querySelector('.sea-article-catalog > .toc');
    if (toggleIcon && tocContent) {
      toggleIcon.addEventListener('click', function () {
        tocContent.classList.toggle('sea-article-catalog-show');
        toggleIcon.classList.toggle('sea-svg-icon-rotate');
      });
    }
  });
</script>
  </main>
  <footer id="sea-footer-container">
  <div class="sea-footer-row">
    <div class="sea-footer-menu-link">
      
        <a
          class="sea-footer-link"
          
          href="mailto:iszhenghailin@gmail.com"
        >
          Email
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://x.com/Carvino_zheng"
        >
          X
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://www.weibo.com/u/2935453535"
        >
          Weibo
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://www.bilibili.com/"
        >
          Bilibili
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://www.zhihu.com/people/arr0g4nce"
        >
          知乎
        </a>
        <span class="sea-footer-link__dot">·</span>
      
    </div>
  </div>
  
  
  <div class="sea-footer-row">
    <div class="sea-footer-copyright">
      <span>©</span>
      
        2025
      
      <span>·</span>
      fishcanf1y
    </div>
    <span class="split-line">|</span>
    <div class="sea-footer-theme-by">
      Theme by <a class="theme" href="https://github.com/hai-zou/hexo-theme-sea" target="_blank">Sea</a>
    </div>
  </div>
</footer>

  
<script src="/js/main.js" defer></script>


<script src="/js/theme.js" defer></script>

</body>
</html>